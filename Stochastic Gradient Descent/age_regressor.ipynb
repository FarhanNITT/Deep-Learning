{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a8551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0897a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 2304)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "X_tr = np.reshape(np.load(\"age_regression_Xtr.npy\"), (-1, 48*48))\n",
    "ytr = np.load(\"age_regression_ytr.npy\")\n",
    "X_te = np.reshape(np.load(\"age_regression_Xte.npy\"), (-1, 48*48))\n",
    "y_te = np.load(\"age_regression_yte.npy\")\n",
    "\n",
    "\n",
    "# splitting the dataset into training and validation - lets keep a 80-20 split as suggested \n",
    "\n",
    "# If using just numpy then : \n",
    "\n",
    "num_datapoints = X_tr.shape[0] # Determine the number of samples  - here 5000\n",
    "\n",
    "split_index = int(0.8 * num_datapoints) # Calculate the index to split at (80% for training, 20% for validation)\n",
    "\n",
    "indices = np.arange(num_datapoints)  # Shuffle the data (important to avoid any potential order biases)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split the indices\n",
    "train_indices = indices[:split_index]   # first 80% of data goes into the training dataset\n",
    "val_indices = indices[split_index:]     # remaining 20% of data goes into the validation dataset\n",
    "\n",
    "# Split the data according to the indices\n",
    "X_train, X_val = X_tr[train_indices], X_tr[val_indices]   \n",
    "y_train, y_val = ytr[train_indices], ytr[val_indices]\n",
    "\n",
    "\n",
    "# Alternate approach to split dataset would be \n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_tr, ytr, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9649816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_age_regressor (X_train,y_train,mini_batch_size,learning_rate,num_epochs):\n",
    "    \n",
    "    #normalizing data\n",
    "    X_train = (X_train - np.mean(X_train, axis=0)) / (np.std(X_train, axis=0) + 1e-8)\n",
    "    y_train = (y_train - np.mean(y_train, axis=0)) / (np.std(y_train, axis=0) + 1e-8)\n",
    "    \n",
    "    # lets start training\n",
    "    \n",
    "    # step1 - initialize weights\n",
    "    \n",
    "    weights = np.zeros(X_train.shape[1])  # equal to 48*48 = 2304\n",
    "    bias = 0.0 # initializing bias to zero -- we compute bias gradient by mean of error values across the batch samples\n",
    "    \n",
    "    # step 2 - randomize order of training set\n",
    "    num_tr_samples = X_train.shape[0]   #4000 images\n",
    "    tr_indices = np.arange(num_tr_samples)   # indices for entire training set - from this we create mini batches\n",
    "    \n",
    "    mse_list = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        #print(\"Initial weights:\", weights)\n",
    "        #print(\"Initial bias:\", bias)\n",
    "\n",
    "        \n",
    "        # shuffle training data before each epoch\n",
    "        np.random.shuffle(tr_indices)\n",
    "        \n",
    "        for num in range(0,num_tr_samples,mini_batch_size):\n",
    "            \n",
    "            batch_index = tr_indices[num: num + mini_batch_size]\n",
    "\n",
    "            \n",
    "            x_batch = X_train[batch_index]\n",
    "            y_batch = y_train[batch_index]\n",
    "            \n",
    "            # computing gradient on selected batch\n",
    "            \n",
    "            error_value = (np.dot(x_batch,weights) + bias) - y_batch \n",
    "            \n",
    "            error_value = np.nan_to_num(error_value, nan=0.0, posinf=np.finfo(np.float64).max, neginf=np.finfo(np.float64).min)\n",
    "\n",
    "            #error_value = np.clip(error_value, -1e5, 1e5)\n",
    "            \n",
    "            mse = np.mean(error_value ** 2)\n",
    "            \n",
    "            # Check for inf or NaN in MSE and skip the batch if encountered\n",
    "            if np.isinf(mse) or np.isnan(mse):\n",
    "                print(f\"Invalid MSE detected at Epoch {epoch+1}, Batch {num//mini_batch_size + 1}. Skipping batch...\")\n",
    "                continue\n",
    "            \n",
    "            mse_list.append(mse)\n",
    "            print(f\"Epoch {epoch+1}, Batch {num//mini_batch_size + 1}, MSE: {mse}\")\n",
    "      #     weight_grad = np.dot(x_batch.T, error_value)/x_batch.shape[0]\n",
    "            weight_grad = np.dot(x_batch.T,(np.dot(x_batch,weights) + bias) - y_batch )/x_batch.shape[0]\n",
    "            \n",
    "            bias_grad = np.mean(error_value)\n",
    "            \n",
    "            # updating weights and bias\n",
    "            \n",
    "            weights -= learning_rate * weight_grad \n",
    "            bias -= learning_rate * bias_grad\n",
    "        \n",
    "\n",
    "    #average_training_mse = np.mean(mse_list)\n",
    "    #print(f\"Average Training MSE: {average_training_mse}\")\n",
    "    #print(\"Last 10 MSE values of gradient descent:\")\n",
    "    #print(mse_list[-10:])\n",
    "    return weights,bias\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1049eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rates = 1e-5 \n",
    "# mini_batch_sizes = 64\n",
    "# num_epochs_testing = 150\n",
    "\n",
    "# wts,bias = train_age_regressor(X_train,y_train,mini_batch_size=mini_batch_sizes, learning_rate=learning_rates,num_epochs=num_epochs_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6bb6f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation an hyperparameter tuning using grid-search\n",
    "\n",
    "def validation(X_train,y_train,X_val,y_val):\n",
    "    \n",
    "    # tune atleast 2 values for each hyperparameter\n",
    "    learning_rates = [1e-5,1e-4,1e-3] \n",
    "    mini_batch_sizes = [32, 64,128]\n",
    "    num_epochs_testing = [50, 100,150]\n",
    "    \n",
    "    best_mse = float('inf')  # setting mse to positive infinity to ensure the first mse calculated becomes the default best value after first iteration and gets updated in the process\n",
    "    best_hyperparams = {}   # dictionary to store the three HP parameters\n",
    "    best_weights, best_bias = None, None\n",
    "    \n",
    "    # normalizing validaton data\n",
    "    X_val = (X_val - np.mean(X_val, axis=0)) / np.std(X_val, axis=0) \n",
    "    y_val = (y_val - np.mean(y_val, axis=0)) / np.std(y_val, axis=0)     \n",
    "    \n",
    "    for rate in learning_rates:\n",
    "        for batch in mini_batch_sizes:\n",
    "            for epoch in num_epochs_testing:\n",
    "                \n",
    "                weights, bias = train_age_regressor(X_train,y_train,mini_batch_size=batch, learning_rate=rate,num_epochs=epoch)\n",
    "                \n",
    "                # once we have the trained weights we validate the model\n",
    "                \n",
    "                y_val_pred = np.dot(X_val, weights) + bias\n",
    "                mse = np.mean((y_val_pred - y_val) ** 2)  # mean squared error to validate prediction\n",
    "                #print(mse)\n",
    "                print(f\"Num_Epoch {epoch}, Batch_size {batch}, Learning_rate {rate}, MSE: {mse}\")\n",
    "                if mse < best_mse:  \n",
    "                    best_mse = mse\n",
    "                    best_hyperparameters = {'num_epochs': epoch,'learning_rate': rate,'mini_batch': batch}\n",
    "                    best_weights,best_bias = weights,bias\n",
    "    \n",
    "    #print(best_mse)\n",
    "    return best_hyperparameters,best_weights,best_bias,best_mse\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8446e26e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_Epoch 50, Batch_size 32, Learning_rate 1e-05, MSE: 0.7980858284148201\n",
      "Num_Epoch 100, Batch_size 32, Learning_rate 1e-05, MSE: 0.7817600759463128\n",
      "Num_Epoch 150, Batch_size 32, Learning_rate 1e-05, MSE: 0.7764923913809932\n",
      "Num_Epoch 50, Batch_size 64, Learning_rate 1e-05, MSE: 0.825059311597272\n",
      "Num_Epoch 100, Batch_size 64, Learning_rate 1e-05, MSE: 0.7982439602620542\n",
      "Num_Epoch 150, Batch_size 64, Learning_rate 1e-05, MSE: 0.7871683315511885\n",
      "Num_Epoch 50, Batch_size 128, Learning_rate 1e-05, MSE: 0.858354726422686\n",
      "Num_Epoch 100, Batch_size 128, Learning_rate 1e-05, MSE: 0.8248488083114183\n",
      "Num_Epoch 150, Batch_size 128, Learning_rate 1e-05, MSE: 0.8079652447471504\n",
      "Num_Epoch 50, Batch_size 32, Learning_rate 0.0001, MSE: 0.768266541636217\n",
      "Num_Epoch 100, Batch_size 32, Learning_rate 0.0001, MSE: 0.7662357760753514\n",
      "Num_Epoch 150, Batch_size 32, Learning_rate 0.0001, MSE: 0.7726601133474943\n",
      "Num_Epoch 50, Batch_size 64, Learning_rate 0.0001, MSE: 0.7692581662422355\n",
      "Num_Epoch 100, Batch_size 64, Learning_rate 0.0001, MSE: 0.7649789122920176\n",
      "Num_Epoch 150, Batch_size 64, Learning_rate 0.0001, MSE: 0.7645158065620998\n",
      "Num_Epoch 50, Batch_size 128, Learning_rate 0.0001, MSE: 0.7777352539608878\n",
      "Num_Epoch 100, Batch_size 128, Learning_rate 0.0001, MSE: 0.7715885337195111\n",
      "Num_Epoch 150, Batch_size 128, Learning_rate 0.0001, MSE: 0.7677323043511067\n",
      "Num_Epoch 50, Batch_size 32, Learning_rate 0.001, MSE: 0.8111897687858998\n",
      "Num_Epoch 100, Batch_size 32, Learning_rate 0.001, MSE: 0.868774148034891\n",
      "Num_Epoch 150, Batch_size 32, Learning_rate 0.001, MSE: 0.9025795459531021\n",
      "Num_Epoch 50, Batch_size 64, Learning_rate 0.001, MSE: 0.7985248087246093\n",
      "Num_Epoch 100, Batch_size 64, Learning_rate 0.001, MSE: 0.8236343826571174\n",
      "Num_Epoch 150, Batch_size 64, Learning_rate 0.001, MSE: 0.8358450180509313\n",
      "Num_Epoch 50, Batch_size 128, Learning_rate 0.001, MSE: 0.77882287155159\n",
      "Num_Epoch 100, Batch_size 128, Learning_rate 0.001, MSE: 0.7868471234771526\n",
      "Num_Epoch 150, Batch_size 128, Learning_rate 0.001, MSE: 0.80671009093568\n"
     ]
    }
   ],
   "source": [
    "best_hyp,best_weights,best_bias,best_mse= validation(X_train,y_train,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47cea7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"best_model_weights2.npy\", best_weights)\n",
    "np.save(\"best_model_bias2.npy\", best_bias)\n",
    "np.save(\"best_model_hyperparameters2\",best_hyp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a894b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on testing data\n",
    "X_te = np.reshape(np.load(\"age_regression_Xte.npy\"), (-1, 48*48))\n",
    "y_te = np.load(\"age_regression_yte.npy\")\n",
    "\n",
    "# normalizing test data to maintain model consistency\n",
    "X_te = (X_te - np.mean(X_te, axis=0)) / np.std(X_te, axis=0)\n",
    "y_te = (y_te - np.mean(y_te, axis=0)) / np.std(y_te, axis=0)\n",
    "\n",
    "# load the best performing weights and bias\n",
    "\n",
    "test_weights = np.load(\"best_model_weights2.npy\")\n",
    "test_bias = np.load(\"best_model_bias2.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a8c400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.7690748379706631\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = np.dot(X_te, test_weights) + test_bias\n",
    "\n",
    "\n",
    "test_mse = np.mean((y_test_pred - y_te) ** 2)\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
